{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tflearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clothing=pd.read_json(\"Documents/amazon_reviews/Clothing_Shoes_and_Jewelry_5.json\",lines=True)\n",
    "df_sports=pd.read_json(\"Documents/amazon_reviews/Sports_and_Outdoors_5.json\",lines=True)\n",
    "df=pd.concat([df_clothing,df_sports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000031887</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great tutu and at a really great pri...</td>\n",
       "      <td>02 12, 2011</td>\n",
       "      <td>A1KLRMWW2FWPL4</td>\n",
       "      <td>Amazon Customer \"cameramom\"</td>\n",
       "      <td>Great tutu-  not cheaply made</td>\n",
       "      <td>1297468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000031887</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I bought this for my 4 yr old daughter for dan...</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>A2G5TCU2WDFZ65</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Very Cute!!</td>\n",
       "      <td>1358553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000031887</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>What can I say... my daughters have it in oran...</td>\n",
       "      <td>01 4, 2013</td>\n",
       "      <td>A1RLQXYNCMWRWN</td>\n",
       "      <td>Carola</td>\n",
       "      <td>I have buy more than one</td>\n",
       "      <td>1357257600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000031887</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>We bought several tutus at once, and they are ...</td>\n",
       "      <td>04 27, 2014</td>\n",
       "      <td>A8U3FAMSJVHS5</td>\n",
       "      <td>Caromcg</td>\n",
       "      <td>Adorable, Sturdy</td>\n",
       "      <td>1398556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000031887</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Thank you Halo Heaven great product for Little...</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>CJ</td>\n",
       "      <td>Grammy's Angels Love it</td>\n",
       "      <td>1394841600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  0000031887  [0, 0]        5   \n",
       "1  0000031887  [0, 0]        5   \n",
       "2  0000031887  [0, 0]        5   \n",
       "3  0000031887  [0, 0]        5   \n",
       "4  0000031887  [0, 0]        5   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  This is a great tutu and at a really great pri...  02 12, 2011   \n",
       "1  I bought this for my 4 yr old daughter for dan...  01 19, 2013   \n",
       "2  What can I say... my daughters have it in oran...   01 4, 2013   \n",
       "3  We bought several tutus at once, and they are ...  04 27, 2014   \n",
       "4  Thank you Halo Heaven great product for Little...  03 15, 2014   \n",
       "\n",
       "       reviewerID                 reviewerName                        summary  \\\n",
       "0  A1KLRMWW2FWPL4  Amazon Customer \"cameramom\"  Great tutu-  not cheaply made   \n",
       "1  A2G5TCU2WDFZ65              Amazon Customer                    Very Cute!!   \n",
       "2  A1RLQXYNCMWRWN                       Carola       I have buy more than one   \n",
       "3   A8U3FAMSJVHS5                      Caromcg               Adorable, Sturdy   \n",
       "4  A3GEOILWLK86XM                           CJ        Grammy's Angels Love it   \n",
       "\n",
       "   unixReviewTime  \n",
       "0      1297468800  \n",
       "1      1358553600  \n",
       "2      1357257600  \n",
       "3      1398556800  \n",
       "4      1394841600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=df[\"reviewText\"].values\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=[clean_str(sent) for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "max_sent_length=100\n",
    "vocab_processor=tflearn.data_utils.VocabularyProcessor(max_sent_length)\n",
    "data=list(vocab_processor.fit_transform(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168948"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocab_processor.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=[i.tolist() for i in data]\n",
    "\n",
    "for lst in data:\n",
    "    try:\n",
    "        while lst[-1]==0:\n",
    "            lst.pop()\n",
    "    except:\n",
    "        pass\n",
    "data = filter(None,data)\n",
    "data = np.array(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 2, 3, 4, 5, 6, 7, 3, 8, 4, 9, 10, 11, 12, 13, 14, 7, 15, 16, 17, 18, 19, 20, 21, 22, 6, 23, 24, 25, 26, 5, 27, 2, 12, 28, 29, 3])\n",
      " list([19, 30, 1, 31, 32, 33, 34, 35, 36, 31, 37, 38, 39, 40, 10, 41, 31, 42, 43, 44, 6, 42, 45, 46, 10, 47, 48, 19, 30, 1, 49, 50, 51, 3, 52, 53, 54, 55, 56, 6, 47, 57, 42, 58, 59, 60, 4, 9, 47, 61, 62, 63, 64, 65, 66, 67, 50, 31, 68, 69, 70, 71])]\n",
      "(574867,)\n"
     ]
    }
   ],
   "source": [
    "print(data[0:2])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "window =2\n",
    "pivot_words = []\n",
    "target_words = []\n",
    "\n",
    "for d in range(data.shape[0]):\n",
    "    pivot_idx = data[d][window:-window]\n",
    "    \n",
    "    for i in range(len(pivot_idx)):\n",
    "        pivot=pivot_idx[i]\n",
    "        targets=np.array([])\n",
    "        neg_target=data[d][i:window+i]\n",
    "        pos_target=data[d][i+window+1:i+window+window+1]\n",
    "        targets=np.append(targets,[neg_target,pos_target]).flatten().tolist()\n",
    "        \n",
    "        for c in range(window*2):\n",
    "            pivot_words.append(pivot)\n",
    "            target_words.append(targets[c])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 4, 4, 4, 4]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_words[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 4.0, 5.0, 2.0, 3.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_words[0:8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
